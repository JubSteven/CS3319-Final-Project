{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import time\n",
    "import copy\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import community as cm\n",
    "\n",
    "def get_scores(edges_pos, edges_neg, A_pred, adj_label):\n",
    "    # get logits and labels\n",
    "    preds_pos = A_pred[edges_pos[:, 0], edges_pos[:, 1]]\n",
    "    preds_neg = A_pred[edges_neg[:, 0], edges_neg[:, 1]]\n",
    "\n",
    "    logits = np.hstack([preds_pos, preds_neg])\n",
    "    labels = np.hstack([np.ones(preds_pos.size(0)), np.zeros(preds_neg.size(0))])\n",
    "\n",
    "    roc_auc = roc_auc_score(labels, logits)\n",
    "    ap_score = average_precision_score(labels, logits)\n",
    "    precisions, recalls, thresholds = precision_recall_curve(labels, logits)\n",
    "    pr_auc = auc(recalls, precisions)\n",
    "\n",
    "    f1s = np.nan_to_num(2 * precisions * recalls / (precisions + recalls))\n",
    "    best_comb = np.argmax(f1s)\n",
    "    f1 = f1s[best_comb]\n",
    "    pre = precisions[best_comb]\n",
    "    rec = recalls[best_comb]\n",
    "    thresh = thresholds[best_comb]\n",
    "\n",
    "    adj_rec = copy.deepcopy(A_pred)\n",
    "    adj_rec[adj_rec < thresh] = 0\n",
    "    adj_rec[adj_rec >= thresh] = 1\n",
    "\n",
    "    labels_all = adj_label.to_dense().view(-1).long()\n",
    "    preds_all = adj_rec.view(-1).long()\n",
    "    recon_acc = (preds_all == labels_all).sum().float() / labels_all.size(0)\n",
    "    results = {\n",
    "        'roc': roc_auc,\n",
    "        'pr': pr_auc,\n",
    "        'ap': ap_score,\n",
    "        'pre': pre,\n",
    "        'rec': rec,\n",
    "        'f1': f1,\n",
    "        'acc': recon_acc,\n",
    "        'adj_recon': adj_rec\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "def sample_graph_det(adj_orig, A_pred, remove_edge_num=100):\n",
    "    if remove_edge_num == 0:\n",
    "        return copy.deepcopy(adj_orig)\n",
    "    orig_upper = sp.triu(adj_orig, 1)\n",
    "    edges = np.asarray(orig_upper.nonzero()).T\n",
    "    if remove_edge_num:\n",
    "        n_remove = remove_edge_num\n",
    "        edge_prob = A_pred[edges.T[0], edges.T[1]]\n",
    "        edge_index_to_remove = np.argpartition(edge_prob, n_remove)[:n_remove]\n",
    "        mask = np.ones(len(edges), dtype=bool)\n",
    "        mask[edge_index_to_remove] = False\n",
    "        edges_pred = edges[mask]\n",
    "    else:\n",
    "        edges_pred = edges\n",
    "\n",
    "    # Recover the edges to [a,b] and [b,a] instead of [a,b]\n",
    "    edges_pred = np.concatenate([edges_pred, edges_pred[:, ::-1]])\n",
    "\n",
    "    return edges_pred\n",
    "\n",
    "\n",
    "def sample_graph_community(adj_orig, A_pred, remove_edge_num=100, tau=0.05):\n",
    "    if remove_edge_num == 0:\n",
    "        return copy.deepcopy(adj_orig)\n",
    "    orig_upper = sp.triu(adj_orig, 1)\n",
    "    edges = np.asarray(orig_upper.nonzero()).T\n",
    "    partition = cm.best_partition(nx.Graph(adj_orig), random_state=42)\n",
    "    # Get the indices for each community index\n",
    "    community_indices = [np.where(np.array(list(partition.values())) == i)[0] for i in set(partition.values())]\n",
    "\n",
    "    if remove_edge_num:\n",
    "        n_remove = remove_edge_num\n",
    "        # Create a list storing the current weight of each node\n",
    "        node_count = np.zeros(len(partition))\n",
    "        edge_index_to_remove = []\n",
    "        for _ in tqdm(range(n_remove)):\n",
    "            # Find the edge with the smallest probability\n",
    "            edge_prob = A_pred[edges.T[0], edges.T[1]]\n",
    "            remaining_edges = [idx for idx in np.argsort(edge_prob) if idx not in edge_index_to_remove]\n",
    "            if len(remaining_edges) == 0:\n",
    "                break\n",
    "            min_edge_idx = remaining_edges[0]\n",
    "            min_edge = edges[min_edge_idx]\n",
    "\n",
    "            # Get the communities of the two nodes\n",
    "            community_i = partition[min_edge[0]]\n",
    "            community_j = partition[min_edge[1]]\n",
    "\n",
    "            # Compute the penalty term\n",
    "            penalty_i = node_count[community_i] / (2 * n_remove)\n",
    "            penalty_j = node_count[community_j] / (2 * n_remove)\n",
    "\n",
    "            # Update the probability of all nodes within the same community as community_i and community_j\n",
    "            A_pred[community_indices[community_i], :] += (1 -\n",
    "                                                          A_pred[community_indices[community_i], :]) * penalty_i * tau\n",
    "            A_pred[community_indices[community_j], :] += (1 -\n",
    "                                                          A_pred[community_indices[community_j], :]) * penalty_j * tau\n",
    "            # A_pred[community_indices[community_i], :] *= (1 + penalty_i * tau)\n",
    "            # A_pred[community_indices[community_j], :] *= (1 + penalty_j * tau)\n",
    "\n",
    "            # Add the edge index to the removal list\n",
    "            edge_index_to_remove.append(min_edge_idx)\n",
    "\n",
    "            # Update the node count for the communities\n",
    "            node_count[community_i] += 1\n",
    "            node_count[community_j] += 1\n",
    "\n",
    "        # Remove the edges with the smallest probabilities\n",
    "        edges_pred = np.delete(edges, edge_index_to_remove, axis=0)\n",
    "    else:\n",
    "        edges_pred = edges\n",
    "\n",
    "    edges_pred = np.concatenate([edges_pred, edges_pred[:, ::-1]])\n",
    "\n",
    "    return edges_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def louvain_clustering(adj, s_rec):\n",
    "    \"\"\"\n",
    "    Performs community detection on a graph using the Louvain method\n",
    "    :param adj: adjacency matrix of the graph\n",
    "    :param s_rec: s hyperparameter for s-regular sparsification\n",
    "    :return: adj_louvain, the Louvain community membership matrix obtained;\n",
    "    nb_communities_louvain, the number of communities; partition, the community\n",
    "    associated with each node from the graph\n",
    "    \"\"\"\n",
    "    graph = nx.Graph(adj)\n",
    "\n",
    "    # Community detection using the Louvain method\n",
    "    partition = cm.best_partition(graph)\n",
    "    communities_louvain = list(partition.values())\n",
    "\n",
    "    # Number of communities found by the Louvain method\n",
    "    nb_communities_louvain = np.max(communities_louvain) + 1\n",
    "\n",
    "    # One-hot representation of communities\n",
    "    communities_louvain_onehot = sp.csr_matrix(np.eye(nb_communities_louvain)[communities_louvain])\n",
    "\n",
    "    # Community membership matrix (adj_louvain[i,j] = 1 if nodes i and j are in the same community)\n",
    "    adj_louvain = communities_louvain_onehot.dot(communities_louvain_onehot.transpose())\n",
    "\n",
    "    # Remove the diagonal\n",
    "    adj_louvain = adj_louvain - sp.eye(adj_louvain.shape[0])\n",
    "\n",
    "    # s-regular sparsification of adj_louvain\n",
    "    adj_louvain = sparsification(adj_louvain, s_rec)\n",
    "\n",
    "    return adj_louvain, nb_communities_louvain, partition\n",
    "\n",
    "\n",
    "def sparsification(adj_louvain, s=1):\n",
    "    \"\"\"\n",
    "    Performs an s-regular sparsification of the adj_louvain matrix (if possible)\n",
    "    :param adj_louvain: the initial community membership matrix\n",
    "    :param s: value of s for s-regular sparsification\n",
    "    :return: s-sparsified adj_louvain matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of nodes\n",
    "    n = adj_louvain.shape[0]\n",
    "\n",
    "    # Compute degrees\n",
    "    degrees = np.sum(adj_louvain, axis=0).getA1()\n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        # Get non-null neighbors of i\n",
    "        edges = sp.find(adj_louvain[i, :])[1]\n",
    "\n",
    "        # More than s neighbors? Subsample among those with degree > s\n",
    "        if len(edges) > s:\n",
    "            # Neighbors of i with degree > s\n",
    "            high_degrees = np.where(degrees > s)\n",
    "            edges_s = np.intersect1d(edges, high_degrees)\n",
    "            # Keep s of them (if possible), randomly selected\n",
    "            removed_edges = np.random.choice(edges_s, min(len(edges_s), len(edges) - s), replace=False)\n",
    "            adj_louvain[i, removed_edges] = 0.0\n",
    "            adj_louvain[removed_edges, i] = 0.0\n",
    "            degrees[i] = s\n",
    "            degrees[removed_edges] -= 1\n",
    "\n",
    "    adj_louvain.eliminate_zeros()\n",
    "\n",
    "    return adj_louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset\n",
    "class GraphData():\n",
    "\n",
    "    def __init__(self, data_path, device=\"cpu\", use_louvain=True, louvain_neighbors=10, louvain_lambda=0.5):\n",
    "        self.data = torch.load(data_path)\n",
    "        self.device = device\n",
    "        self.use_louvain = use_louvain\n",
    "        self.louvain_neighbors = louvain_neighbors\n",
    "        self.louvain_lambda = louvain_lambda\n",
    "        self.init_data()\n",
    "\n",
    "    def init_data(self):\n",
    "        self.x, self.y, self.edge_index, self.val_mask, self.train_mask = self.data.x, self.data.y, self.data.edge_index, self.data.val_mask, self.data.train_mask\n",
    "\n",
    "        # Filter out the validation and training data\n",
    "        self.x_train = self.x[self.train_mask]\n",
    "        self.edge_index_train = torch.stack(\n",
    "            [edge for edge in self.edge_index.permute(1, 0) if self.train_mask[edge[0]] and self.train_mask[edge[1]]])\n",
    "        self.edge_index_val = torch.stack(\n",
    "            [edge for edge in self.edge_index.permute(1, 0) if self.val_mask[edge[0]] and self.val_mask[edge[1]]])\n",
    "\n",
    "        # Adjacency matrix contains all the edges in the graph\n",
    "        # NOTE: train_edges here can be all the edges or the training edges only\n",
    "        self.train_edges = self.edge_index\n",
    "        # ! self.x ought to be modified if train_edges is not all edges\n",
    "        self.adj_train = self.build_adj(self.train_edges, self.x.shape[0]).numpy()\n",
    "\n",
    "        if self.use_louvain:\n",
    "            self.louvain_adj, _, _ = louvain_clustering(self.adj_train, self.louvain_neighbors)\n",
    "            self.adj_train_louvain = self.adj_train + self.louvain_adj * self.louvain_lambda\n",
    "\n",
    "        self.adj_train_norm = self.normalize_adj(self.adj_train)\n",
    "        self.adj_train_louvain_norm = self.normalize_adj(self.adj_train_louvain)\n",
    "\n",
    "        # ! self.x ought to be modified if train_edges is not all edges\n",
    "        self.adj_label = self.build_adj_label(self.train_edges, self.x.shape[0])\n",
    "\n",
    "        self.val_edges = self.edge_index_val\n",
    "        self.val_edges_false = self.generate_false_edges(self.val_edges.shape[0])\n",
    "\n",
    "    def generate_false_edges(self, num_edges):\n",
    "        \"\"\"\n",
    "            Generates num_edges false edges for the graph\n",
    "        \"\"\"\n",
    "        false_edges = set()\n",
    "\n",
    "        while len(false_edges) < num_edges:\n",
    "            src_node = np.random.randint(0, self.x.shape[0])\n",
    "            dst_node = np.random.randint(0, self.x.shape[0])\n",
    "            if src_node != dst_node and not self.adj_train_norm[src_node, dst_node]:\n",
    "                false_edges.add((src_node, dst_node))\n",
    "\n",
    "        false_edges = torch.tensor(list(false_edges)).to(self.device)\n",
    "        return false_edges\n",
    "\n",
    "    def sparse_to_tuple(self, sparse_mx):\n",
    "        if not sp.isspmatrix_coo(sparse_mx):\n",
    "            sparse_mx = sparse_mx.tocoo()\n",
    "        coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "        values = sparse_mx.data\n",
    "        shape = sparse_mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    def build_adj(self, edge_idx, num_verts, half=False):\n",
    "        \"\"\"\n",
    "            Input:\n",
    "                edge_idx: [torch.Tensor] a tensor of shape (2, num_edges) representing the edge indices of the graph\n",
    "                num_verts: the number of nodes in the graph\n",
    "            Returns:\n",
    "                adjacency_matrix: an adjacency matrix built from edge_idx \n",
    "        \"\"\"\n",
    "        adjacency_matrix = torch.zeros((num_verts, num_verts))\n",
    "\n",
    "        # Iterate over each edge and set the corresponding entries in the adjacency matrix\n",
    "        for i in range(edge_idx.size(1)):\n",
    "            src_node = edge_idx[0, i]\n",
    "            dst_node = edge_idx[1, i]\n",
    "            adjacency_matrix[src_node, dst_node] = 1\n",
    "            if not half:\n",
    "                adjacency_matrix[dst_node, src_node] = 1\n",
    "\n",
    "        return adjacency_matrix\n",
    "\n",
    "    def build_adj_label(self, edge_idx, num_verts, half=False):\n",
    "        \"\"\"\n",
    "            Input:\n",
    "                edge_idx: [torch.Tensor] a tensor of shape (2, num_edges) representing the edge indices of the graph\n",
    "                num_verts: the number of nodes in the graph\n",
    "            Returns:\n",
    "                adj_label: [torch.sparse.FloatTensor] a sparse matrix representing the adjacency matrix label of the graph (half)\n",
    "        \"\"\"\n",
    "        adj = self.build_adj(edge_idx, num_verts, half)\n",
    "        adj_label = adj + torch.eye(num_verts)\n",
    "        adj_label = adj_label.to_sparse()\n",
    "\n",
    "        return adj_label\n",
    "\n",
    "    def normalize_adj(self, adj_train):\n",
    "        \"\"\"\n",
    "            Input:\n",
    "                adj_train: [np.ndarray] a sparse matrix representing the adjacency matrix of the graph\n",
    "            Returns:\n",
    "                adj_norm: [torch.sparse.FloatTensor] a normalized version of the adjacency matrix\n",
    "        \"\"\"\n",
    "        adj_ = sp.coo_matrix(adj_train)\n",
    "        adj_.setdiag(1)\n",
    "        rowsum = np.array(adj_.sum(1))\n",
    "        degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "        adj_norm = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "        adj_norm_tuple = self.sparse_to_tuple(adj_norm)\n",
    "        adj_norm = torch.sparse.FloatTensor(torch.LongTensor(adj_norm_tuple[0].T), torch.FloatTensor(adj_norm_tuple[1]),\n",
    "                                            torch.Size(adj_norm_tuple[2]))\n",
    "\n",
    "        return adj_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Define the VGAE model\n",
    "class VGAE(nn.Module):\n",
    "    \"\"\"\n",
    "        The GVAE is adapted from https://github.com/zhao-tong/GAug/blob/master/vgae/models.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, adj, dim_in, dim_h, dim_z, use_gae=False, adj_louvain=None):\n",
    "        super(VGAE, self).__init__()\n",
    "\n",
    "        self.dim_z = dim_z\n",
    "        self.gae = use_gae\n",
    "        if adj_louvain is not None:\n",
    "            self.base_gcn = GraphConvSparse(dim_in, dim_h, adj_louvain)\n",
    "        else:\n",
    "            self.base_gcn = GraphConvSparse(dim_in, dim_h, adj)\n",
    "\n",
    "        self.gcn_mean = GraphConvSparse(dim_h, dim_z, adj, activation=False)\n",
    "        self.gcn_logstd = GraphConvSparse(dim_h, dim_z, adj, activation=False)\n",
    "\n",
    "    def encode(self, X):\n",
    "        # TODO: modify X to fuse basic graph features\n",
    "        hidden = self.base_gcn(X)\n",
    "        self.mean = self.gcn_mean(hidden)\n",
    "        if self.gae:\n",
    "            # graph auto-encoder\n",
    "            return self.mean\n",
    "        else:\n",
    "            # variational graph auto-encoder\n",
    "            self.logstd = self.gcn_logstd(hidden)\n",
    "            gaussian_noise = torch.randn_like(self.mean)\n",
    "            sampled_z = gaussian_noise * torch.exp(self.logstd) + self.mean\n",
    "            return sampled_z\n",
    "\n",
    "    def decode(self, Z):\n",
    "        A_pred = Z @ Z.T\n",
    "        return A_pred\n",
    "\n",
    "    def forward(self, X, F=None):\n",
    "        if F is not None:\n",
    "            # TODO design different fusing strategies\n",
    "            if isinstance(F, np.ndarray):\n",
    "                F = torch.from_numpy(F).float().to(X.device)\n",
    "            X = torch.cat([X, F], dim=1)\n",
    "        Z = self.encode(X)\n",
    "        A_pred = self.decode(Z)\n",
    "        return A_pred\n",
    "\n",
    "\n",
    "class GraphConvSparse(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, adj, activation=True):\n",
    "        super(GraphConvSparse, self).__init__()\n",
    "        self.weight = self.glorot_init(input_dim, output_dim)\n",
    "        self.adj = adj\n",
    "        self.activation = activation\n",
    "\n",
    "    def glorot_init(self, input_dim, output_dim):\n",
    "        init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "        initial = torch.rand(input_dim, output_dim) * 2 * init_range - init_range\n",
    "        return nn.Parameter(initial)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs @ self.weight\n",
    "        x = self.adj @ x\n",
    "        if self.activation:\n",
    "            return F.elu(x)\n",
    "        else:\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(cfg, graph_data, model, structural_features=None):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg[\"lr\"])\n",
    "    adj_t = graph_data.adj_train\n",
    "    norm_w = adj_t.shape[0]**2 / float((adj_t.shape[0]**2 - adj_t.sum()) * 2)\n",
    "    pos_weight = torch.FloatTensor([float(adj_t.shape[0]**2 - adj_t.sum()) / adj_t.sum()]).to(cfg[\"device\"])\n",
    "\n",
    "    # move input data and label to gpu if needed\n",
    "    features = graph_data.x.to(cfg[\"device\"])\n",
    "    adj_label = graph_data.adj_label.to_dense().to(cfg[\"device\"])\n",
    "\n",
    "    best_vali_criterion = 0.0\n",
    "    best_state_dict = None\n",
    "    model.train()\n",
    "\n",
    "    train_bar = tqdm(range(cfg[\"epoch\"]))\n",
    "    for epoch in train_bar:\n",
    "        A_pred = model(X=features, F=structural_features)\n",
    "        optimizer.zero_grad()\n",
    "        loss = norm_w * F.binary_cross_entropy_with_logits(A_pred, adj_label, pos_weight=pos_weight)\n",
    "        if not cfg[\"use_gae\"]:\n",
    "            kl_divergence = 0.5 / A_pred.size(0) * (1 + 2 * model.logstd - model.mean**2 -\n",
    "                                                    torch.exp(2 * model.logstd)).sum(1).mean()\n",
    "            loss -= kl_divergence\n",
    "\n",
    "        A_pred = torch.sigmoid(A_pred).detach().cpu()\n",
    "        r = get_scores(graph_data.val_edges, graph_data.val_edges_false, A_pred, graph_data.adj_label)\n",
    "\n",
    "        if r[cfg[\"criterion\"]] > best_vali_criterion:\n",
    "            best_vali_criterion = r[cfg[\"criterion\"]]\n",
    "            best_state_dict = copy.deepcopy(model.state_dict())\n",
    "            r_test = r\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_bar.set_description(\n",
    "            f\"E: {epoch+1} | L: {loss.item():.4f} | A: {r['acc']:.4f} | ROC: {r['roc']:.4f} | AP: {r['ap']:.4f} | F1: {r['f1']:.4f}\"\n",
    "        )\n",
    "\n",
    "    print(\"Training completed. Final results: test_roc: {:.4f} test_ap: {:.4f} test_f1: {:.4f} test_recon_acc: {:.4f}\".\n",
    "          format(r_test['roc'], r_test['ap'], r_test['f1'], r_test['acc']))\n",
    "\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    # Dump the best model\n",
    "    torch.save(model.state_dict(), f'aug_model.pt')\n",
    "    return model\n",
    "\n",
    "\n",
    "def gen_graphs(cfg, graph_data, model, structural_features=None):\n",
    "    adj_orig = graph_data.adj_train\n",
    "\n",
    "    if cfg[\"use_gae\"]:\n",
    "        pickle.dump(adj_orig, open(f'graphs/graph_0_gae.pkl', 'wb'))\n",
    "    else:\n",
    "        pickle.dump(adj_orig, open(f'graphs/graph_0.pkl', 'wb'))\n",
    "\n",
    "    features = graph_data.x.to(cfg[\"device\"])\n",
    "    for i in range(cfg[\"gen_graphs\"]):\n",
    "        with torch.no_grad():\n",
    "            A_pred = model(features, structural_features)\n",
    "\n",
    "        A_pred = torch.sigmoid(A_pred).detach().cpu()\n",
    "        adj_recon = A_pred.numpy()\n",
    "        np.fill_diagonal(adj_recon, 0)\n",
    "\n",
    "        if cfg[\"use_gae\"]:\n",
    "            filename = f'graphs/graph_{i+1}_logits_gae.pkl'\n",
    "        else:\n",
    "            filename = f'graphs/graph_{i+1}_logits.pkl'\n",
    "\n",
    "        pickle.dump(adj_recon, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "def main(data_path, cfg):\n",
    "    torch.manual_seed(cfg[\"seed\"])\n",
    "\n",
    "    graph_data = GraphData(data_path, cfg[\"device\"]) \n",
    "\n",
    "    model = VGAE(\n",
    "        adj=graph_data.adj_train_norm,\n",
    "        adj_louvain=graph_data.adj_train_louvain_norm if cfg[\"use_louvain\"] else None,\n",
    "        dim_in=graph_data.x.shape[1],\n",
    "        dim_h=cfg[\"dim_h\"],\n",
    "        dim_z=cfg[\"dim_z\"],\n",
    "        use_gae=cfg[\"use_gae\"],\n",
    "    ).to(cfg[\"device\"])\n",
    "\n",
    "    if cfg[\"pretrained\"]:\n",
    "        model.load_state_dict(torch.load(cfg[\"pretrained\"]))\n",
    "    else:\n",
    "        model = train_model(cfg, graph_data, model)\n",
    "\n",
    "    if cfg[\"gen_graphs\"] > 0:\n",
    "        # Generate graphs\n",
    "        gen_graphs(cfg, graph_data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def from_graph():\n",
    "    data_loader = GraphData(\"data\\data.pt\")\n",
    "\n",
    "    graph_aug = \"graph_1_logits.pkl\"  # NOTE: You can change this to a different graph\n",
    "\n",
    "    with open(os.path.join(\"graphs\", graph_aug), \"rb\") as f:\n",
    "        adj = pickle.load(f)\n",
    "\n",
    "    # No AUG - baseline (no delete edges)\n",
    "    # [0.792, 0.796, 0.808, 0.796, 0.794, 0.800]\n",
    "\n",
    "    remove_edges = [100, 200, 300, 400, 500, 600]\n",
    "    edge_list = []\n",
    "    for enum in remove_edges:\n",
    "        edges = sample_graph_community(data_loader.adj_train, adj, enum, tau=0.005)\n",
    "        edge_list.append(edges.T.reshape(-1).tolist())\n",
    "\n",
    "    # NOTE: Don't change this, used for generating the submission csv\n",
    "    df = pd.DataFrame(edge_list).fillna(-1).astype(int)\n",
    "    # fill those empty units with -1 (don't change it)\n",
    "    df.insert(0, 'ID', list(range(len(edge_list))))\n",
    "    df.to_csv('submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
